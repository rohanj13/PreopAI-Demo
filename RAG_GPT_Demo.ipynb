{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqrH0G07nYKk"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nTHcHOgxoV4"
      },
      "outputs": [],
      "source": [
        "!pip3 install -qU bs4 tiktoken openai langchain langchain-community pinecone pypdf tqdm dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksVMdUysRY95"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "\n",
        "pdf_folder_path = \"references/\" #clinical document location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MMxrndhQTZ-"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "loader = PyPDFDirectoryLoader(pdf_folder_path)\n",
        "dataset = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H2bsyVT0ClM"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "for doc in dataset:\n",
        "    data.append({\n",
        "        'reference': doc.metadata['source'].replace('rtdocs/', 'https://'),\n",
        "        'text': doc.page_content\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWXruhm5S5SQ"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "# create the length function\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FCYmaKGS87y"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    length_function=tiktoken_len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-PQwiN1TXF0"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "chunks = []\n",
        "\n",
        "for idx, record in enumerate(tqdm(data)):\n",
        "    texts = text_splitter.split_text(record['text'])\n",
        "    chunks.extend([{\n",
        "        'id': str(uuid4()),\n",
        "        'text': texts[i],\n",
        "        'chunk': i,\n",
        "        'reference': record['reference']\n",
        "    } for i in range(len(texts))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhaAONVNTmxX"
      },
      "source": [
        "Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pq7s6XcTkdW"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "embed_model = \"text-embedding-3-small\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOpzgtL_nezR"
      },
      "source": [
        "Vector Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcPrqVIiTql5"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "\n",
        "pc = Pinecone(\n",
        "    api_key=pinecone_api_key, #Pinecone API\n",
        "    # environment=\"gcp-starter\"\n",
        ")\n",
        "index_name = \"preopai-index-py\"\n",
        "\n",
        "if not pc.has_index(index_name):\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        vector_type=\"dense\",\n",
        "        dimension=1536,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(\n",
        "            cloud=\"aws\",\n",
        "            region=\"us-east-1\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATElJYPCT8Cx"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import datetime\n",
        "from time import sleep\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for i in tqdm(range(0, len(chunks), batch_size)):\n",
        "    i_end = min(len(chunks), i+batch_size)\n",
        "    meta_batch = chunks[i:i_end]\n",
        "    ids_batch = [x['id'] for x in meta_batch]\n",
        "    texts = [x['text'] for x in meta_batch]\n",
        "    try:\n",
        "        res = client.embeddings.create(input=texts, model=embed_model)\n",
        "    except:\n",
        "        done = False\n",
        "        while not done:\n",
        "            sleep(5)\n",
        "            try:\n",
        "                res = client.embeddings.create(input=texts, model=embed_model)\n",
        "                done = True\n",
        "            except:\n",
        "                pass\n",
        "    embeds = [record.embedding for record in res.data]\n",
        "    meta_batch = [{\n",
        "        'text': x['text'],\n",
        "        'chunk': x['chunk'],\n",
        "        'reference': x['reference']\n",
        "    } for x in meta_batch]\n",
        "    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
        "    index.upsert(vectors=to_upsert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qqJl4zhUKOD"
      },
      "source": [
        "Retrieval Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCtzC1vGUIGo"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(\n",
        "    api_key=pinecone_api_key, #Pinecone API\n",
        "    # environment=\"gcp-starter\"\n",
        ")\n",
        "index_name = \"preopai-index-py\"\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UmChHaJURTi"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "query = str(\"38/Chinese/Female\\\n",
        "Allergy to aspirin, paracetamol, penicillin - rashes and itchiness \\\n",
        "ExSmoker—smoked 10 years ago/Occasional Drinker \\\n",
        "LMP: last month\\\n",
        "Wt 94.7 Ht 166.3 BMI 34.2 BP 127/81 HR 88 SpO2 100% on RA \\\n",
        "Coming in for BILATERAL REVISION FESS, REVISION SEPTOPLASTY, ADENOIDECTOMY, AND BILATERAL INFERIOR TURBINOPLASTIES/SEVERE OSA ON CPAP \\\n",
        "=== PAST MEDICAL HISTORY ===\\\n",
        "1. Severe OSA on CPAP—AHI 58—CPAP settings: AutoCPAP (4–15) cmH2O, without humidifier/Chinstrap\\\n",
        "2. Right persistent Sinusitis\\\n",
        "3. Allergic rhinitis\\\n",
        "4. Adenoid hypertrophy\\\n",
        "5. High BMI\\\n",
        "6. Asthma—f/u GP, last seen 3 months ago for attack—on PRN ventolin—Does not use ventolin at all—No previous admissions/ intubations for asthma\\\n",
        "7. Diabetes—HbA1C 9.4%, Last seen outpatient doctor >1 year ago.\\\n",
        "No history of HTN/ HLD/ IHD/ CVA\\\n",
        "=== SURGICAL HISTORY===\\\n",
        "Tonsillectomy > 10 years ago mild PONV\\\n",
        "===Investigations===\\\n",
        "Hb 13.0 TW 4 Plt 392\\\n",
        "INR PT APTT normal\\\n",
        "Na 134 K3.4 Cr 77 Glu 13\\\n",
        "ECG NSR\\\n",
        "CXR NAD\\\n",
        "=== MEDICATIONS===\\\n",
        "Ventolin PRN\\\n",
        "LMP; Last menstrual period, Wt; Weight\") #clinical query\n",
        "\n",
        "res = client.embeddings.create(\n",
        "    input=[query],\n",
        "    model=embed_model\n",
        ")\n",
        "\n",
        "xq = res.data[0].embedding\n",
        "res = index.query(\n",
        "    namespace=\"__default__\",\n",
        "    vector=xq, \n",
        "    top_k=10, \n",
        "    include_metadata=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4nHGt-GUYAz"
      },
      "source": [
        "Response Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZRL3nGNUYa5"
      },
      "outputs": [],
      "source": [
        "contexts = [item['metadata']['text'] for item in res['matches']]\n",
        "augmented_query = \"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\"+query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZGwGH1OUbGh"
      },
      "outputs": [],
      "source": [
        "print(augmented_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ4361Jum_rC"
      },
      "source": [
        "LLM Integration (GPT 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouifbSEOnHpK"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"o3-mini\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \n",
        "     \"You are the anesthesiologist seeing this patient in the preoperative clinic 2 weeks before the date of operation. The patients have already taken their routine preoperative\\\n",
        "      investigations and the findings are listed within the clinical summary.\\\n",
        "      Your role is to evaluate the clinical summary and give the preoperative anesthesia instructions for the following patient targeted to your fellow medical colleagues. You are to\\\n",
        "      follow strictly the guidelines.\\\n",
        "      Your instructions should consist of the following components:\\\n",
        "      1. Provide a traffic light status for the surgery. If there is a risk and the patient needs to be seen by a Doctor or a Nurse, its red, if further tests are required, its yellow; if the patient is healthy and ready for surgery, its green.\\\n",
        "      2. Fasting instructions - list instructions based on the number of hours before the time of the listed surgery\\\n",
        "      3. Suitability for preoperative carbohydrate loading — yes/no.\\\n",
        "      4. Medication instructions — name each medication and give the instructions for the day of the operation and days leading up to the operation as required.\\\n",
        "      5. Any instructions for the healthcare team—for example, preoperative blood group matching, arranging for preoperative dialysis, or standby post-operative high\\\n",
        "      dependency/ICU beds.\\\n",
        "      6. Provide the RCRI, ASA, DASI and STOP-BANG scores for the patient. If you cannot calculate it, provide the extra information you need to calculate it.\\\n",
        "      Your instructions are the final instructions, explain the reasoning for your \\\n",
        "      if you are uncertain, explain what further information you require.\\\n",
        "      If the medical condition is already optimized, there is no need to offer further optimization. If there\\\n",
        "      are no relevant instructions in any of the above categories, leave it blank and write NA\"}, #System Prompt\n",
        "    {\"role\": \"user\", \"content\": augmented_query},\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIny4TvmnU4C"
      },
      "outputs": [],
      "source": [
        "print(response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
